{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(FileName):\n",
    "    xls_path = os.path.join(FileName)\n",
    "    return pd.ExcelFile(xls_path)\n",
    "\n",
    "data = pd.read_excel(load_data(\"trace.xlsx\"), '1')\n",
    "data = shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.externals.joblib import dump\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)\n",
    "X_train = train_set.iloc[:,0:12]\n",
    "y_train = train_set.iloc[:,12]\n",
    "\n",
    "X_test = test_set.iloc[:,0:12]\n",
    "y_test = test_set.iloc[:,12]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test =  scaler.transform(X_test)\n",
    "\n",
    "train_data = X_train\n",
    "train_data_label = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['std_scaler.bin']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.preprocessing\n",
    "\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "\n",
    "#save the encoder for tester\n",
    "dump(scaler, 'std_scaler.bin', compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        device_specs = pd.DataFrame()\n",
    "  \n",
    "        # append columns to an empty DataFrame\n",
    "        device_specs['Name'] = ['fridge_01', 'smart_tv_01', 'pc_01']\n",
    "        device_specs['total_memory'] = [97, 600, 200]\n",
    "        device_specs['available_memory'] = [2200, 775, 300]\n",
    "        \n",
    "        self.state = {\n",
    "            \"task_id\": \"task_0001\",\n",
    "            \"device_specs\": device_specs,\n",
    "        }\n",
    "    \n",
    "    def get_state(self):\n",
    "        \n",
    "        return self.state\n",
    "        \n",
    "    def update_state(self, task_id, ):\n",
    "        \n",
    "        self.state = get_data_from_devies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment1:\n",
    "    \n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.done = False\n",
    "        \n",
    "        return self.data[self.t, :]\n",
    "    \n",
    "    def step(self, act):\n",
    "        reward = 0\n",
    "        \n",
    "        # act = 0: unoccupied, 1: occupied\n",
    "        if act == self.label.iloc[self.t]:\n",
    "            reward +=1 \n",
    "        else:\n",
    "            reward -=1\n",
    "  \n",
    "        # set next time\n",
    "        self.t += 1\n",
    "        \n",
    "        return self.data[self.t, :], reward, self.done # obs, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from plotly import tools\n",
    "from plotly.graph_objs import *\n",
    "from plotly.offline import init_notebook_mode, iplot, iplot_mpl\n",
    "import numpy as np\n",
    "\n",
    "def train_dqn(env):\n",
    "\n",
    "    class Q_Network(chainer.Chain):\n",
    "\n",
    "        def __init__(self, input_size, hidden_size, output_size):\n",
    "            super(Q_Network, self).__init__(\n",
    "                fc1 = L.Linear(input_size, hidden_size),\n",
    "                fc2 = L.Linear(hidden_size, hidden_size),\n",
    "                fc3 = L.Linear(hidden_size, output_size)\n",
    "            )\n",
    "\n",
    "        def __call__(self, x):\n",
    "            h = F.relu(self.fc1(x))\n",
    "            h = F.relu(self.fc2(h))\n",
    "            y = self.fc3(h)\n",
    "            return y\n",
    "\n",
    "        def reset(self):\n",
    "            self.zerograds()\n",
    "\n",
    "    Q = Q_Network(input_size=12, hidden_size=100, output_size=2) # 4 features, 2 actions\n",
    "    Q_ast = copy.deepcopy(Q)\n",
    "    optimizer = chainer.optimizers.Adam()\n",
    "    optimizer.setup(Q)\n",
    "\n",
    "    epoch_num = 20\n",
    "    step_max = len(env.data)-1\n",
    "    memory_size = 800\n",
    "    batch_size = 20\n",
    "    epsilon = 1.0\n",
    "    epsilon_decrease = 1e-3\n",
    "    epsilon_min = 0.1\n",
    "    start_reduce_epsilon = 200\n",
    "    train_freq = 10\n",
    "    update_q_freq = 20\n",
    "    gamma = 0.97\n",
    "    show_log_freq = 5\n",
    "\n",
    "    memory = []\n",
    "    total_step = 0\n",
    "    total_rewards = []\n",
    "    total_losses = []\n",
    "\n",
    "    start = time.time()\n",
    "    for epoch in range(epoch_num):\n",
    "\n",
    "        pobs = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        total_loss = 0\n",
    "\n",
    "        while not done and step < step_max:\n",
    "\n",
    "            # select act\n",
    "            pact = np.random.randint(2)\n",
    "            if np.random.rand() > epsilon:\n",
    "                pact = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n",
    "                pact = np.argmax(pact.data)\n",
    "\n",
    "            # act\n",
    "            obs, reward, done = env.step(pact)\n",
    "\n",
    "            # add memory\n",
    "            memory.append((pobs, pact, reward, obs, done))\n",
    "            if len(memory) > memory_size:\n",
    "                memory.pop(0)   \n",
    "            \n",
    "            # train or update q\n",
    "            if len(memory) == memory_size:\n",
    "                if total_step % train_freq == 0:\n",
    "                    shuffled_memory = np.random.permutation(memory)\n",
    "                    memory_idx = range(len(shuffled_memory))\n",
    "                    for i in memory_idx[::batch_size]:\n",
    "                        batch = np.array(shuffled_memory[i:i+batch_size])\n",
    "                        \n",
    "                        b_pobs = np.array(batch[:, 0].tolist(), dtype=np.float32).reshape(batch_size, -1)\n",
    "                        b_pact = np.array(batch[:, 1].tolist(), dtype=np.int32)\n",
    "                        \n",
    "                        b_reward = np.array(batch[:, 2].tolist(), dtype=np.int32)\n",
    "                        b_obs = np.array(batch[:, 3].tolist(), dtype=np.float32).reshape(batch_size, -1)\n",
    "                        b_done = np.array(batch[:, 4].tolist(), dtype=np.bool)\n",
    "                        q = Q(b_pobs)\n",
    "                        maxq = np.max(Q_ast(b_obs).data, axis=1)\n",
    "                        target = copy.deepcopy(q.data)\n",
    "                        for j in range(batch_size):\n",
    "                            target[j, b_pact[j]] = b_reward[j]+gamma*maxq[j]*(not b_done[j])\n",
    "                        Q.reset()\n",
    "                        loss = F.mean_squared_error(q, target)\n",
    "                        total_loss += loss.data\n",
    "                        loss.backward()\n",
    "                        optimizer.update()\n",
    "\n",
    "                if total_step % update_q_freq == 0:\n",
    "                    Q_ast = copy.deepcopy(Q)\n",
    "\n",
    "            # epsilon\n",
    "            if epsilon > epsilon_min and total_step > start_reduce_epsilon:\n",
    "                epsilon -= epsilon_decrease\n",
    "\n",
    "            # next step\n",
    "            total_reward += reward\n",
    "            pobs = obs\n",
    "            step += 1\n",
    "            total_step += 1\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "        total_losses.append(total_loss)\n",
    "\n",
    "        if (epoch+1) % show_log_freq == 0:\n",
    "            log_reward = sum(total_rewards[((epoch+1)-show_log_freq):])/show_log_freq\n",
    "            log_loss = sum(total_losses[((epoch+1)-show_log_freq):])/show_log_freq\n",
    "            elapsed_time = time.time()-start\n",
    "            print('\\t'.join(map(str, [epoch+1, epsilon, total_step, log_reward, log_loss, elapsed_time])))\n",
    "            start = time.time()\n",
    "            \n",
    "    return Q, total_losses, total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "5\t0.0999999999999992\t35035\t1412.6\t8022.357741842791\t868.3909432888031\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "10\t0.0999999999999992\t70070\t1596.6\t9739.635128593445\t933.5282363891602\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "15\t0.0999999999999992\t105105\t1958.6\t10680.776064751297\t907.0877966880798\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "Q, total_losses, total_rewards = train_dqn(Environment1(train_data,train_data_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainer import serializers\n",
    "serializers.save_npz('SavedModels/Q.model', Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_reward(total_losses, total_rewards):\n",
    "\n",
    "    figure = tools.make_subplots(rows=1, cols=2, subplot_titles=('loss', 'reward'), print_grid=False)\n",
    "    figure.append_trace(Scatter(y=total_losses, mode='lines', line=dict(color='skyblue')), 1, 1)\n",
    "    figure.append_trace(Scatter(y=total_rewards, mode='lines', line=dict(color='orange')), 1, 2)\n",
    "    figure['layout']['xaxis1'].update(title='epoch')\n",
    "    figure['layout']['xaxis2'].update(title='epoch')\n",
    "    figure['layout'].update(height=400, width=900, showlegend=False)\n",
    "    iplot(figure)\n",
    "\n",
    "plot_loss_reward(total_losses, total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
